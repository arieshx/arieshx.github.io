---
layout: post
title: starXin 一个简单的深度学习框架
category: DP框架
description: 一个写给自己的框架
---

框架结果：训练mnist，用cpu模式，36s；用GPU模式27s。
##　关于整体架构
我们要实现一个深度学习框架，其实就是想要一个能够在GPU上计算的工具，最基础的是定义数据，这数据可以在cpu或者gpu上，数据的低层计算我们应该用C++实现

初步构想：

数据： 切换cpu/gpu，有设备id，有数据类型（此框架未定义，只有一种数据类型），数据底层计算用c++，cuda实现

假设我们实现之后要声明这样一个数据，用以下的函数，那么需要根据需求定义出对应的方法。
```python
auto *out = new DLArrayHandle()
DLArrayAlloc(shape,ndim,ctx, out);
```
数据定义
```python
typedef enum {
    kCPU = 1,
    kGPU = 2,
}DLDeviceType;

typedef struct {
    int device_id;
    DLDeviceType device_type;
}DLContext;

// 无缝切换的数据结构
typedef struct {
void *data;    // 数据
DLContext ctx; // 上下文
int ndim;   // 维度
int64_t *shape;  // 每一个维度的大小
}DLArray;
```
有了数据，还需要根据数据可能需要的操作来定义相关的方法
```python
对数据分配空间
int DLArrayAlloc(const index_t *shape, index_t ndim, DLContext ctx,
                 DLArrayHandle *out);
#　有分配空间，就有释放空间
int DLArrayFree(DLArrayHandle handle);
# 从a数据赋值到b数据，这包括了跨cpu，gpu之间的赋值
int DLArrayCopyFromTo(DLArrayHandle from, DLArrayHandle to,
                      DLStreamHandle stream); 
# 给数据赋值
int DLGpuArraySet(DLArrayHandle arr, float value);                         
# 还有常用的数据之间的加减乘除等需要的运算，需要实现的是gpu版本和cpu版本的
。。。。

```
定义相关的方法，其中在gpu上计算时，需要用到cuda编程，这可以参考cuda编程的入门教程。举例：
```python
__global__
void matrix_elementwise_division(const float *a, const float *b, float *result, int n) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index < n) {
        result[index] = a[index] / b[index];
    }
}

int DLGpuMatrixElementwiseDiv(const DLArrayHandle matA, const DLArrayHandle matB,
                              DLArrayHandle output) {
    int n = 1;
    for (int i = 0; i < output->ndim; i++) {
        n = n * output->shape[i];
    }
    const float *data_A = (const float *) matA->data;
    const float *data_B = (const float *) matB->data;
    float *data_output = (float *) output->data;

    int threads_per_block = 1024;
    int num_blocks = (n + threads_per_block - 1) / threads_per_block;

    matrix_elementwise_division << < num_blocks, threads_per_block >> > (data_A, data_B,
            data_output, n);
    return 0;

}
```
## python 接口
c++端定义好了数据操作，并将其编译成.so，供python端调用
python端需要定义
- 数据类型
- 自动求导
- 网络中需要用到的层
- 优化器


## 关于动态图和静态图

### pytorch 是动态图
简单理解：
torch中的tensor，既包含了网络中流动的数据，也包含了计算图的信息，也就是说，计算图是在哪儿体现呢，就在tensor的属性grad_fn体现，实例代码如下：
```angularjs
x = torch.tensor([[1, 2, 3]], dtype = torch.float)
W_x = torch.rand([1,3], requires_grad=True)
h = torch.tensor([[1, 1, 1, 4]], dtype = torch.float)
W_h = torch.rand([1,4], requires_grad=True)


i2h = torch.mm(W_x, x.t())
h2h = torch.mm(W_h, h.t())

next_h = h2h + i2h

next_next_h = next_h.tanh()

next_next_h.backward()

print(x.grad)
```
用户创建的tensor是叶节点，也就是说 x,h,W他们是孤立 的节点；声明i2h时，遍会定义到目前为止的计算图，这信息也就是包含在i2h节点信息中，
```python
i2h.grad_fn : <MmBackward object at 0x7fd0182242b0>
# grad_fn 是一个backward对象， 也就是一个Mm对象创建了i2h
i2h.grad_fn.next_functions : <class 'tuple'>: ((<AccumulateGrad object at 0x7fd018224630>, 0), (None, 0))
            .requires_grad : True
# i2h 本身是MmBackward,它的下一个函数是W_x, x.t()的，就这样一层层直到叶节点。也就是AccumulateGrad 

```
每声明一个节点，都会计算到此时的节点值，和生成到此时的grad_fn信息，直到某个tensor调用backward，那么该函数就可以根据grad_fn信息一层层计算下去。
###　 tensorflow 是静态图
简单理解：
静态图就是先定义好图，图中 的节点还没有数据值，只是定义好了操作，需要运行的时候feed数据，然后就可以快速执行。本框架实现的便是静态图。
以此框架starXin搭建3层全连接网络为例
```python
    # 直接定义的varible和 Parameter 是叶子节点，W1,....b3都是叶子节点， logit是生成的节点    
    X = ad.Variable(name="X")
    y = ad.Variable(name='y')

    loss, W1, b1, W2, b2, W3, b3, logit = build_graph(X, y, input_size, hid_1_size, hid_2_size, output_size)
    # 定义优化器，优化器内部在做什么呢？构建计算图，主要是为需要梯度的叶子节点定义好计算图，这样就有了计算梯度的方式，
    optimizer = th.optim.SGD(loss, params=[W1, b1, W2, b2, W3, b3], lr=lr, use_gpu=use_gpu)
    # 运算，feed数据 ，这一步之前，计算图都已经定义好，给了数据，就可以按照之前排好的拓扑排序计算出答案，然后再根据优化策略，将需要更新梯度的节点变化即可。
    loss_now = optimizer.step(feed_dict={X: X_batch, y: y_batch})
    
```

