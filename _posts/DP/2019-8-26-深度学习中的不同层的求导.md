---
layout: post
title: 深度学习中不同层的求导
category: 深度学习
description: 深度学习中不同层的求导
---

持续汇总更新

- DNN层
- CNN层
- RNN层
- pooling层
- 激活函数层：relu，sigmoid，
- 常见的损失函数求导
- #

## DNN 层

![img](https://pic2.zhimg.com/80/v2-d003e09134b0b053294504c6a3283afd_hd.jpg)

## CNN层

[CNN反向求导](https://zhuanlan.zhihu.com/p/40951745  )

## RNN

略 ，BPTT，比较复杂

## pooling

很容易理解，参考 [pooling](https://blog.csdn.net/qq_21190081/article/details/72871704 )

## 激活函数

### relu

先从最简单的开始，Relu激活在高等数学上的定义为连续（局部）不可微的函数，它的公式为 
![img](https://pic1.zhimg.com/80/v2-785de5cba744620011f2d3df50acaf62_hd.png)

其在x=0x=0处是不可微的，但是在深度学习框架的代码中为了解决这个直接将其在x=0x=0处的导数置为1，所以它的导数也就变为了 
![img](https://pic2.zhimg.com/80/v2-906e110cb9dd5ffcff073a9544c59bd9_hd.png)

### sigmoid

### softmax

1. softmax函数

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26%E8%AE%BEX%3D%5Bx_1%2Cx_2%2C%5Ccdots%2Cx_n%5D%2CY%3Dsoftmax%28X%29%3D%5By_1%2Cy_2%2C%5Ccdots%2Cy_n%5D%5C%5C%26%E5%88%99y_i%3D%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum%5Climits_%7Bj%3D1%7D%5E%7Bn%7De%5E%7Bx_j%7D%7D%2C%E6%98%BE%E7%84%B6%5Csum%5Climits_%7Bi%3D1%7D%5Eny_i%3D1+%5Cend%7Balign%7D)

![[公式]](https://www.zhihu.com/equation?tex=%E4%BE%8B%E5%A6%82%EF%BC%9AX%3D%5B2%2C3%5D%EF%BC%8C%E5%88%99Y%3Dsoftmax%28X%29%3D%5B%5Cfrac%7Be%5E2%7D%7Be%5E2%2Be%5E3%7D%2C%5Cfrac%7Be%5E3%7D%7Be%5E2%2Be%5E3%7D%5D)

numpy代码：

```python3
import numpy as np
def softmax(x):
    x = np.exp(x)/np.sum(np.exp(x))
    return x
print(softmax([2,3]))
```

结果：[ 0.26894142 0.73105858]



2. softmax函数求导 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+y_i%7D%7B%5Cpartial+x_j%7D)

(1)当 ![[公式]](https://www.zhihu.com/equation?tex=i%3Dj) 时

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Cfrac%7B%5Cpartial+y_i%7D%7B%5Cpartial+x_j%7D%26%3D%5Cfrac%7B%5Cpartial+y_i%7D%7B%5Cpartial+x_i%7D+%5C%5C+%26%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_i%7D%28%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_ke%5E%7Bx_k%7D%7D%29+%5C%5C%26%3D%5Cfrac%7B%28e%5E%7Bx_i%7D%29%5E%7B%27%7D%28%5Csum_ke%5E%7Bx_k%7D%29-e%5E%7Bx_i%7D%28%5Csum_ke%5E%7Bx_k%7D%29%5E%7B%27%7D%7D%7B%28%5Csum_ke%5E%7Bx_k%7D%29%5E2%7D+%5C%5C+%26%3D%5Cfrac%7Be%5E%7Bx_i%7D%5Ccdot%28%5Csum_ke%5E%7Bx_k%7D%29-e%5E%7Bx_i%7D%5Ccdot+e%5E%7Bx_i%7D%7D%7B%28%5Csum_ke%5E%7Bx_k%7D%29%5E2%7D+%5C%5C+%26%3D%5Cfrac%7Be%5E%7Bx_i%7D%5Ccdot%28%5Csum_ke%5E%7Bx_k%7D%29%7D%7B%28%5Csum_ke%5E%7Bx_k%7D%29%5E2%7D-%5Cfrac+%7Be%5E%7Bx_i%7D%5Ccdot+e%5E%7Bx_i%7D%7D%7B%28%5Csum_ke%5E%7Bx_k%7D%29%5E2%7D+%5C%5C+%26%3D%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_ke%5E%7Bx_k%7D%7D-%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_ke%5E%7Bx_k%7D%7D%5Ccdot+%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_ke%5E%7Bx_k%7D%7D%5C%5C%26%3Dy_i-y_i%5Ccdot+y_i%5C%5C%26%3Dy_i%281-y_i%29+%5Cend%7Balign%7D)

(2)当 ![[公式]](https://www.zhihu.com/equation?tex=i%5Cne+j) 时

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+y_i%7D%7B%5Cpartial+x_j%7D%26%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_j%7D%28%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_ke%5E%7Bx_k%7D%7D%29+%5C%5C%26%3D%5Cfrac%7B%28e%5E%7Bx_i%7D%29%5E%7B%27%7D%28%5Csum_ke%5E%7Bx_k%7D%29-e%5E%7Bx_i%7D%28%5Csum_ke%5E%7Bx_k%7D%29%5E%7B%27%7D%7D%7B%28%5Csum_ke%5E%7Bx_k%7D%29%5E2%7D+%5C%5C%26%3D%5Cfrac%7B0%5Ccdot%28%5Csum_ke%5E%7Bx_k%7D%29-e%5E%7Bx_i%7D%5Ccdot+e%5E%7Bx_j%7D%7D%7B%28%5Csum_ke%5E%7Bx_k%7D%29%5E2%7D+%5C%5C%26%3D%5Cfrac%7B-e%5E%7Bx_i%7D%5Ccdot+e%5E%7Bx_j%7D%7D%7B%28%5Csum_ke%5E%7Bx_k%7D%29%5E2%7D+%5C%5C%26%3D-%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_ke%5E%7Bx_k%7D%7D%5Ccdot+%5Cfrac%7Be%5E%7Bx_j%7D%7D%7B%5Csum_ke%5E%7Bx_k%7D%7D%5C%5C%26%3D-y_i%5Ccdot+y_j+%5Cend%7Baligned%7D)

综上所述： ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+y_i%7D%7B%5Cpartial+x_j%7D%3D%5Cleft%5C%7B+%5Cbegin%7Baligned%7D+%26%3Dy_i-y_iy_i+%2C%E5%BD%93i%3Dj%5C%5C+%26%3D+0-y_i%5Ccdot+y_j+%EF%BC%8C+%E5%BD%93+i+%5Cne+j+%5C%5C+%5Cend%7Baligned%7D+%5Cright.)

所以 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Y%7D%7B%5Cpartial+X%7D%3Ddiag%28Y%29-Y%5ET%5Ccdot+Y) (当Y的shape为（1，n）时)

3. 实际应用：为了防止溢出，事先把x减去最大值。最大值是有效数据，其他值溢不溢出可管不了，也不关心。

```python3
import numpy as np
def softmax(x):
    #减去最大值
    x-=np.max(x)
    x = np.exp(x)/np.sum(np.exp(x))
    return x
print(softmax([2,3]))
```



## 损失函数

### 交叉熵

[交叉熵的导数公式](https://zhuanlan.zhihu.com/p/75971638 ) 

![img](https://pic2.zhimg.com/80/v2-f718b2ad294053cc488dad3de6c372a5_hd.jpg)

**二分类**

在二分的情况下，模型最后需要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为p和1-p。此时表达式为：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7DJ+%3D+%E2%88%92%5By%5Ccdot+log%28p%29%2B%281%E2%88%92y%29%5Ccdot+log%281%E2%88%92p%29%5D%5Cend%7Balign%7D+%5C%5C)

其中：

\- y——表示样本的label，正类为1，负类为0

\- p——表示样本预测为正的概率

**多分类**

多分类的情况实际上就是对二分类的扩展：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7DJ+%3D+-%5Csum_%7Bi%3D1%7D%5EC+y_%7Bi%7D%5Clog%28p_%7Bi%7D%29%5Cend%7Balign%7D+%5C%5C)

其中： - C——类别的数量； - y——指示变量（0或1）,如果该类别和样本的类别相同就是1，否则是0； - p——对于观测样本属于类别c的预测概率。

现在我们利用这个表达式计算上面例子中的损失函数值：

例子1：

![img](https://pic2.zhimg.com/80/v2-e010052b907fde905cbeeea46fd2b1fd_hd.jpg)

例子2：

![img](https://pic1.zhimg.com/80/v2-4ed8fa6dd2d14a5b76244b00cd54bf64_hd.jpg)

可以发现，交叉熵损失函数可以捕捉到**模型1**和**模型2**预测效果的差异