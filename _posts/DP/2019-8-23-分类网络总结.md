---
layout: post
title: 深度学习中的常见的分类网络
category: 深度学习
description: 深度学习中的常见的分类网络
---

- LeNet5:（5层）
- AlexNet
- ZFNet
- VGG
- inception 
- ResNet



## lenet

![img](https://upload-images.jianshu.io/upload_images/5773600-12b0338e90a2ad98?imageMogr2/auto-orient/strip%7CimageView2/2/w/802/format/webp)

将BP算法应用到神经网络中，手写数字体识别问题

## AlexNet：

（2012）（7层）（参数量增大，模型复杂）

![img](https://upload-images.jianshu.io/upload_images/5773600-d14081657ebc9689?imageMogr2/auto-orient/strip%7CimageView2/2/w/791/format/webp)

1、数据增强（水平翻转、随机裁剪、平移变换、颜色光照变换）

2、DropOut：防止过拟合，加快网络训练，有模型集成的效果

3、引入ReLU激活函数，而避免或抑制网络训练时的梯度消失现象，网络模型的收敛速度会相对稳定

4、 Local Response Normalization：Local Response Normalization要硬翻译的话是局部响应归一化，简称LRN，实际就是利用临近的数据做归一化。这个策略贡献了1.2%的Top-5错误率。

5、 Overlapping Pooling：Overlapping的意思是有重叠，即Pooling的步长比Pooling Kernel的对应边要小。这个策略贡献了0.3%的Top-5错误率。 （这里的pool层是3*3，stride=2）

6、多GPU并行

## ZFNet：

（7层）主要贡献在于在一定程度上解释了卷积神经网络为什么有效，以及如何提高网络的性能（参数量和AlexNet类似）

![img](https://upload-images.jianshu.io/upload_images/5773600-a75c236d671d3d75?imageMogr2/auto-orient/strip%7CimageView2/2/w/797/format/webp)

1、使用了反卷积网络，可视化了特征图。通过特征图证明了浅层网络学习到了图像的边缘、颜色和纹理特征，高层网络学习到了图像的抽象特征；

2、根据特征可视化，提出AlexNet第一个卷积层卷积核太大，导致提取到的特征模糊；

3、通过几组遮挡实验，对比分析找出了图像的关键部位；

4、论证了更深的网络模型，具有更好的性能。



## VGG

![img](https://upload-images.jianshu.io/upload_images/5773600-1ed764217ec1f9ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

核心思想：小核，堆叠

**vgg16**

主要分成5个stages,22333，13个卷积层，16的意思应该是加上3个FC层。每个stage后面都跟着一个pool来减小尺寸

![img](https://img-blog.csdn.net/20170927185636108)

![img](https://img-blog.csdn.net/20170924202742074)

1、Vgg16有16层网络，AlexNet只有8层；

2、在训练和测试时使用了多尺度做数据增强。

3、使用3*3代替5*5的卷积核

## Inceptionnet

### v1

（相比于vgg，层数更深，但是参数量却降低）

进一步增加了网络模型的深度和宽度，但是单纯的在VGG16的基础上增加网络的宽度深度会带来以下缺陷：

1、过多的参数容易引起过拟合；

2、层数的加深，容易引起梯度消失现象。

特点：

1、多头梯度回传

2、增加宽度

3、全连接层之前，做mean-pooling（细节与整体信息的权衡），卷积到全连接层

inception后的聚合是concat操作

![img](https://upload-images.jianshu.io/upload_images/5773600-17672bf41067b136?imageMogr2/auto-orient/strip%7CimageView2/2/w/844/format/webp)

### v2

1、两个3*3的卷积代替5*5的大卷积，在降低参数的同时建立了更多的非线性变换，使得 CNN 对特征的学习能力更强

两个3*3能够代替5*5的原因：

（1）感受野相同

（2）在具有同样感受野的情况，2个3*3 的卷积核，增加了网络深度，且经过两次非线性变换，具有更好的特征学习能力

（3）2个3*3卷积核，减少参数量

2、 提出Batch Normalization

### v3

![img](https://upload-images.jianshu.io/upload_images/5773600-05858cbc3d253ee5?imageMogr2/auto-orient/strip%7CimageView2/2/w/591/format/webp)

![img](https://upload-images.jianshu.io/upload_images/5773600-c9daf25a74dfa9c6?imageMogr2/auto-orient/strip%7CimageView2/2/w/443/format/webp)

1、是引入了 Factorization into small convolutions 的思想，将一个较大的二维卷积拆成两个较小的一维卷积，比如将7´7卷积拆成1´7卷积和7´1卷积，或者将3´3卷积拆成1´3卷积和3´1卷积，如上图所示。一方面节约了大量参数，加速运算并减轻了过拟合（比将7´7卷积拆成1´7卷积和7´1卷积，比拆成3个3´3卷积更节约参数），同时增加了一层非线性扩展模型表达能力。论文中指出，这种非对称的卷积结构拆分，其结果比对称地拆为几个相同的小卷积核效果更明显，可以处理更多、更丰富的空间特征，增加特征多样性。

能够用1*3和3*1代替3*3的原因：

假如是两个高斯核，是完全等价的。如果不是的话，也可以近似等价。此外，还可以降低参数量。

2、另一方面，Inception V3 优化了 Inception Module 的结构，现在 Inception Module 有35´35、17´17和8´8三种不同结构。这些 Inception Module 只在网络的后部出现，前部还是普通的卷积层。并且 Inception V3 除了在 Inception Module 中使用分支，还在分支中使用了分支（8´8的结构中），可以说是Network In Network In Network。

### v4

Inception V4 相比 V3 主要是结合了微软的 **ResNet**，将错误率进一步减少到 **3.08%**。

![img](https://img-blog.csdn.net/20170425212622874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGlhbW9uam95X3pvbmU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

## ResNet

模型的直接加深引入的退化问题，网络不能够训练,

为什么说解决了退化问题？

1 说梯度消失，但是有BN，

2  由于非线性激活函数Relu的存在，每次输入到输出的过程都几乎是不可逆的（信息损失）。我们很难从输出反推回完整的输入。因此，可以认为Residual Learning的初衷，其实是让模型的内部结构至少有恒等映射的能力。以保证在堆叠网络的过程中，网络至少不会因为继续堆叠而产生退化！

但如果把网络设计为**H(x) = F(x) + x，即直接把恒等映射作为网络的一部分**。就可以把问题转化为**学习一个残差函数F(x) = H(x) - x.**

![img](https://upload-images.jianshu.io/upload_images/5773600-9e2746b038c045e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/577/format/webp)

1、反复增加原始信号，而且有利于梯度的反向传播（网络深度加深时，会出现梯度消失）

2、残差，输出与输入的差异，element-wise的加叠

3、bootleNeck：减少计算量

![img](https://upload-images.jianshu.io/upload_images/5773600-fae125b6df6c84aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/785/format/webp)

4、虚线连接：增加深度  (维度不一致时)

short connect 三种方式：

- 使用恒等映射，如果residual block的输入输出维度不一致，对增加的维度用0来填充；
- 在block输入输出维度一致时使用恒等映射，不一致时使用线性投影以保证维度一致；
- 对于所有的block均使用线性投影。

5、计算量小（想比于同级别的vgg19）

**RexNet-v2:**

v2的结构与v1的区别如下：

v2比v1好的原因：

1）反向传播基本符合假设，信息传递无阻碍；

2）BN层作为pre-activation，起到了正则化的作用；

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20170306114206883?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3NwYmE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## ResNeXt

![img](https://upload-images.jianshu.io/upload_images/5971313-76517d84179727d8.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/495/format/webp)

![img](https://upload-images.jianshu.io/upload_images/5971313-3e432e8d585723c2.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

ResNeXt网络结果的变形如上图

上面图中显示了三种ResNeXt网络模块的变形。它们在数学计算上是完全等价的，而第三种包含有Group convolution操作的正是最终ResNeXt网络所采用的操作。

## DenseNet

（CVPR2017）（参数和计算比resnet更好，性能更优）

![img](https://upload-images.jianshu.io/upload_images/5067993-6c3dbdc5ece30919?imageMogr2/auto-orient/strip%7CimageView2/2/w/488/format/webp)

![img](https://upload-images.jianshu.io/upload_images/5067993-5cea78b50a78df6b?imageMogr2/auto-orient/strip%7CimageView2/2/w/960/format/webp)

![img](https://upload-images.jianshu.io/upload_images/5067993-bbd7b1856ac7cb1f?imageMogr2/auto-orient/strip%7CimageView2/2/w/962/format/webp)

启发：

作者提出的随机深度网络（Deep networks with stochastic depth），就是在ResNet的基础上，随机drop一些层，可以显著提高泛化性能。

1、网络并不一定要是一个递进层级结构

2、drop层，不影响收敛和预测结果，说明结构存在冗余

densenet

1，减轻了消失梯度（梯度消失）

2，加强了特征的传递

3，更有效地利用了特征

4，一定程度上较少了参数数量

5、文中还用到dropout操作来随机减少分支，避免过拟合

dense net 能够减少feature的个数，源于原来的网络结构是递进氏的，不能重复提取特征，所以需要更多的features，一次提取尽可能多的特征。

但是现在是密集连接，每一层都能看到前面的所有层，所以可以减少feature的个数，在需要的时候从前面的特征中学习。

密集连接后，采用concat操作聚合

最后是global average pooling，后接分类的全连接。

## SENet

（加入SE 的模块，参数量和计算量基本没有增加）（CVPR2018）

https://www.jianshu.com/p/7244f64250a8

1、做通道信息的整合，之前都是做空间的（关注于卷积核的设计）

3个步骤：

1、squeeze：使用global average pooling作为Squeeze操作。



![img](https:////upload-images.jianshu.io/upload_images/13555903-d9bfed41b57bc8b5.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/640/format/webp)

image

2、Excitation：紧接着两个Fully Connected 层组成一个Bottleneck结构去建模通道间的相关性，并输出和输入特征同样数目的权重。我们首先将特征维度降低到输入的1/16，然后经过ReLu激活后再通过一个Fully Connected 层升回到原来的维度。这样做比直接用一个Fully Connected层的好处在于：

```
       1）具有更多的非线性，可以更好地拟合通道间复杂的相关性；

       2）极大地减少了参数量和计算量。然后通过一个Sigmoid的门获得0~1之间归一化的权重，
```

3、Reweight：通过一个Scale的操作来将归一化后的权重加权到每个通道的特征上。

**SE-ResNet：**

在Addition前对分支上Residual的特征进行了特征重标定。如果对Addition后主支上的特征进行重标定，由于在主干上存在0~1的scale操作，在网络较深BP优化时就会在靠近输入层容易出现梯度消散的情况，导致模型难以优化。

## 轻量化神经网络结构

SqueezeNet、 MobileNet、 ShuffleNet、Xception（Xception 并不是真正意义上的轻量化模型，只是其借鉴 depth-wise convolution，而 depth-wise convolution 又是上述几个轻量化模型的关键点）

![img](https://upload-images.jianshu.io/upload_images/5773600-9ae2a6cf03ec318f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/552/format/webp)

SqueezeNet：

最主要的是fire module结构，fire module 包含两部分：squeeze 层+expand 层

squeeze 层，就是 1*1 卷积

Expand 层分别用 1*1 和 3*3 卷积，然后 concat

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20160315232750747)

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20160316002407347)

小结：

1 Fire module 与 GoogLeNet 思想类似，采用 1*1 卷积对 feature map 的维数进行「压缩」，从而达到减少权值参数的目的；

2 采用与 VGG 类似的思想——堆叠的使用卷积，这里堆叠的使用 Fire module，SqueezeNet 与 GoogLeNet 和 VGG 的关系很大！

3、最红参数进行了压缩。才小于0.5M，并且网络本身参数就这么小。

### MobileNet

主要是采用深度可分离卷积的思想（并非原创，而是借鉴），两阶段，depthwise conv 和point conv（1*1卷积）

![img](https://upload-images.jianshu.io/upload_images/2207461-08871dbeb0394ab3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](https://upload-images.jianshu.io/upload_images/2207461-5a7fc58ed8cc6fee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](https://upload-images.jianshu.io/upload_images/5773600-405957643f342f57.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/479/format/webp)

1. 核心思想是采用 depth-wise convolution 操作，在相同的权值参数数量的情况下，相较于 standard convolution 操作，可以减少数倍的计算量，从而达到提升网络运算速度的目的。

2. depth-wise convolution 的思想非首创，借鉴于 2014 年一篇博士论文：《L. Sifre. Rigid-motion scattering for image classification. hD thesis, Ph. D. thesis, 2014》

3. 采用 depth-wise convolution 会有一个问题，就是导致「信息流通不畅」，即输出的 feature map 仅包含输入的 feature map 的一部分，在这里，MobileNet 采用了 point-wise convolution 解决这个问题。在后来，ShuffleNet 采用同样的思想对网络进行改进，只不过把 point-wise convolution 换成了 channel shuffle

### ShuffleNet