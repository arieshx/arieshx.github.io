---
layout: post
title: 深度学习中不同的normalization
category: 深度学习
description: 深度学习中不同的normalization
---

## BN

batch normalization

操作及其反向传播： https://zhuanlan.zhihu.com/p/75973111

### DNN中的BN

![img](https://pic1.zhimg.com/80/v2-e9d6fe6d521a670ba25ef66b31d586e4_hd.jpg)

*图 6. 前向神经网络中的 BatchNorm* 对于前向神经网络来说，BatchNorm 在计算隐层某个神经元 k 激活的规范值的时候，对应的神经元集合 S 范围是如何划定呢？图 6 给出了示意。因为对于 Mini-Batch 训练方法来说，根据 Loss 更新梯度使用 Batch 中所有实例来做，所以对于神经元 k 来说，假设某个 Batch 包含 n 个训练实例，那么每个训练实例在神经元 k 都会产生一个激活值，也就是说 Batch 中 n 个训练实例分别通过同一个神经元 k 的时候产生了 n 个激活值，BatchNorm 的集合 S 选择入围的神经元就是这 n 个同一个神经元被 Batch 不同训练实例激发的激活值。划定集合 S 的范围后，Normalization 的具体计算过程与前文所述计算过程一样，采用公式 3 即可完成规范化操作。

### CNN中的BN

![img](https://pic2.zhimg.com/80/v2-3a73eb4caaa220d4a86f3ac5666cb1c9_hd.jpg)

只要清楚CNN经过BN要学习多少个γ和β就知道BN的实现：

答：CNN为（Batch * 长 * 宽 * 通道C）经过BN要学习C组参数

### RNN中的BN

没有什么用，略。

### BN的问题

1. 如果 Batch Size 太小，则 BN 效果明显下降。 BN 是严重依赖 Mini-Batch 中的训练实例的，如果 Batch Size 比较小则任务效果有明显的下降。那么多小算是太小呢？图 10 给出了在 ImageNet 数据集下做分类任务时，使用 ResNet 的时候模型性能随着 BatchSize 变化时的性能变化情况，可以看出当 BatchSize 小于 8 的时候开始对分类效果有明显负面影响。之所以会这样，是因为在小的 BatchSize 意味着数据样本少，因而得不到有效统计量，也就是说噪音太大。
2. 对于有些像素级图片生成任务来说，BN 效果不佳； 对于图片分类等任务，只要能够找出关键特征，就能正确分类，这算是一种粗粒度的任务，在这种情形下通常 BN 是有积极效果的。但是对于有些输入输出都是图片的像素级别图片生成任务，比如图片风格转换等应用场景，使用 BN 会带来负面效果，这很可能是因为在 Mini-Batch 内多张无关的图片之间计算统计量，弱化了单张图片本身特有的一些细节信息。
3. NN 等动态网络使用 BN 效果不佳且使用起来不方便 对于 RNN 来说，尽管其结构看上去是个静态网络，但在实际运行展开时是个动态网络结构，因为输入的 Sequence 序列是不定长的，这源自同一个 Mini-Batch 中的训练实例有长有短。对于类似 RNN 这种动态网络结构，BN 使用起来不方便，因为要应用 BN，那么 RNN 的每个时间步需要维护各自的统计量，而 Mini-Batch 中的训练实例长短不一，这意味着 RNN 不同时间步的隐层会看到不同数量的输入数据，而这会给 BN 的正确使用带来问题。假设 Mini-Batch 中只有个别特别长的例子，那么对较深时间步深度的 RNN 网络隐层来说，其统计量不方便统计而且其统计有效性也非常值得怀疑。另外，如果在推理阶段遇到长度特别长的例子，也许根本在训练阶段都无法获得深层网络的统计量。综上，在 RNN 这种动态网络中使用 BN 很不方便，而且很多改进版本的 BN 应用在 RNN 效果也一般。
4. 训练时和推理时统计量不一致 对于 BN 来说，采用 Mini-Batch 内实例来计算统计量，这在训练时没有问题，但是在模型训练好之后，在线推理的时候会有麻烦。因为在线推理或预测的时候，是单实例的，不存在 Mini-Batch，所以就无法获得 BN 计算所需的均值和方差，一般解决方法是采用训练时刻记录的各个 Mini-Batch 的统计量的数学期望，以此来推算全局的均值和方差，在线推理时采用这样推导出的统计量。虽说实际使用并没大问题，但是确实存在训练和推理时刻统计量计算方法不一致的问题。

## **Layer** Normalization

### DNN

![img](https://pic2.zhimg.com/80/v2-dff68f254979f8da014532a2b3f2d381_hd.jpg)

只要清楚DNN经过BN要学习多少个γ和β就知道BN的实现：

答：DNN为（Batch * 特证数m）经过BN要学习Batch组参数

### CNN

![img](https://pic4.zhimg.com/80/v2-375434e01e836e73944889819ca1cb4b_hd.jpg)

只要清楚CNN经过BN要学习多少个γ和β就知道BN的实现：

答：CNN为（Batch * 长 * 宽 * 通道C）经过BN要学习B组参数

### RNN

![img](https://pic3.zhimg.com/80/v2-572d9bdffa0bc757b34dac40ae054846_hd.jpg)

而 Layer Normalization 这种在同隐层内计算统计量的模式就比较符合 RNN 这种动态网络，目前在 RNN 中貌似也只有 LayerNorm 相对有效，但 Layer Normalization 目前看好像也只适合应用在 RNN 场景下，在 CNN 等环境下效果是不如 BatchNorm 或者 GroupNorm 等模型的

## Instance Normalization

DNN和RNN不可以

### CNN

![img](https://pic3.zhimg.com/80/v2-a844c7d6e2e0e897530f2b4e1f44fc12_hd.jpg)

只要清楚CNN经过BN要学习多少个γ和β就知道BN的实现：

答：CNN为（Batch * 长 * 宽 * 通道C）经过BN要学习B * C组参数

Instance Normalization 对于一些图片生成类的任务比如图片风格转换来说效果是明显优于 BN 的，但在很多其它图像类任务比如分类等场景效果不如 BN。

## **Group** Normalization

### CNN

![img](https://pic4.zhimg.com/80/v2-672b3aeffc640fdf80d20be8434c00c7_hd.jpg)

只要清楚CNN经过BN要学习多少个γ和β就知道BN的实现：

答：CNN为（Batch * 长 * 宽 * 通道C）经过BN要学习B * k组参数

在要求 Batch Size 比较小的场景下或者物体检测／视频分类等应用场景下效果是优于 BN 的

## SN

自适配归一化(Switchable Normalization)

https://zhuanlan.zhihu.com/p/75972864

## 分析

1、BN的提出者说是解决了 ICS（Internal Covariate Shift）问题，但是后续研究表明，并非如此。

有研究表情，是因为BN平滑了损失曲面，因此相对与原有的凹凸不平的损失曲面来说，更加容易优化。

使用情况：

1、对于 RNN 的神经网络结构来说，目前只有 LayerNorm 是相对有效的；

2、如果是 GAN 等图片生成或图片内容改写类型的任务，可以优先尝试 InstanceNorm；

3、如果使用场景约束 BatchSize 必须设置很小，无疑此时考虑使用 GroupNorm；

4、而其它任务情形应该优先考虑使用 BatchNorm。