---
layout: post
title: 损失函数总结
category: 深度学习
description: 损失函数总结
---

- CE loss
- WCE Loss
- Focal loss
- Dice loss
- IOU loss
- EMD loss

https://blog.csdn.net/m0_37477175/article/details/83004746

https://zhuanlan.zhihu.com/p/45200767

## Dice loss

一般应用在分割任务，我在文本检测的分类损失时使用过。

其实优化的目标很类似与F1-score，dice coefficient 源于二分类，本质上是衡量两个样本的重叠部分。该指标范围从0到1，其中“1”表示完整的重叠。

![[公式]](https://www.zhihu.com/equation?tex=D+i+c+e%3D%5Cfrac%7B2%7CA+%5Ccap+B%7C%7D%7B%7CA%7C%2B%7CB%7C%7D)

损失函数：

![[公式]](https://www.zhihu.com/equation?tex=L%3D+1+-+Dice)

Dice有两种，一种是soft，一种是hard，hard计算存在一个阈值处理，不可导，用作损失函数的时候就只能用soft，不然梯度无法回传。

soft的tf实现：

```python
def dice_coe(output, target, loss_type='jaccard', axis=(1, 2, 3), smooth=1e-5):
    """Soft dice (Sørensen or Jaccard) coefficient for comparing the similarity
    of two batch of data, usually be used for binary image segmentation
    i.e. labels are binary. The coefficient between 0 to 1, 1 means totally match.

    Parameters
    -----------
    output : Tensor
        A distribution with shape: [batch_size, ....], (any dimensions).
    target : Tensor
        The target distribution, format the same with `output`.
    loss_type : str
        ``jaccard`` or ``sorensen``, default is ``jaccard``.
    axis : tuple of int
        All dimensions are reduced, default ``[1,2,3]``.
    smooth : float
        This small value will be added to the numerator and denominator.
            - If both output and target are empty, it makes sure dice is 1.
            - If either output or target are empty (all pixels are background), dice = ```smooth/(small_value + smooth)``, then if smooth is very small, dice close to 0 (even the image values lower than the threshold), so in this case, higher smooth can have a higher dice.

    Examples
    ---------
    >>> outputs = tl.act.pixel_wise_softmax(network.outputs)
    >>> dice_loss = 1 - tl.cost.dice_coe(outputs, y_)

    References
    -----------
    - `Wiki-Dice <https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient>`__

    """
    inse = tf.reduce_sum(output * target, axis=axis)
    if loss_type == 'jaccard':
        l = tf.reduce_sum(output * output, axis=axis)
        r = tf.reduce_sum(target * target, axis=axis)
    elif loss_type == 'sorensen':
        l = tf.reduce_sum(output, axis=axis)
        r = tf.reduce_sum(target, axis=axis)
    else:
        raise Exception("Unknow loss_type")
    dice = (2. * inse + smooth) / (l + r + smooth)
    dice = tf.reduce_mean(dice)
    return dice
```

hard：

```python
def dice_hard_coe(output, target, threshold=0.5, axis=[1,2,3], smooth=1e-5):
    output = tf.cast(output > threshold, dtype=tf.float32) # thresholding
    target = tf.cast(target > threshold, dtype=tf.float32) # thresholding
    inse = tf.reduce_sum(tf.multiply(output, target), axis=axis)
    l = tf.reduce_sum(output, axis=axis)
    r = tf.reduce_sum(target, axis=axis)
    hard_dice = (2. * inse + smooth) / (l + r + smooth)
    hard_dice = tf.reduce_mean(hard_dice)
    return hard_dice
```

值得注意的是，dice loss比较适用于样本极度不均的情况，一般的情况下，使用 dice loss 会对反向传播造成不利的影响，容易使训练变得不稳定。

## EMD loss

线性规划问题： https://blog.csdn.net/wangdonggg/article/details/32329879

深度学习应用： https://zhuanlan.zhihu.com/p/33194024

Wasserstein 距离，也叫Earth Mover's Distance，推土机距离，简称EMD，用来表示两个分布的相似程度。

假设有两个工地P和Q，P工地上有m堆土，Q工地上有n个坑，现在要将P工地上的m堆土全部移动到Q工地上的n个坑中，所做的最小的功。

每堆土我们用一个二元组来表示(p,w)，p表示土堆的中心，w表示土的数量。则这两个工地可表示为：

![[公式]](https://www.zhihu.com/equation?tex=P%3D%5C%7B%28p_1%2Cw_%7Bp1%7D%29%2C%5Cdots%28p_m%2Cw_%7Bpm%7D%29%5C%7D)

![[公式]](https://www.zhihu.com/equation?tex=Q%3D%5C%7B%28q_1%2Cw_%7Bq1%7D%29%2C%5Cdots%28q_n%2Cw_%7Bqn%7D%29%5C%7D)

每个土堆中心 ![[公式]](https://www.zhihu.com/equation?tex=p_i) 到每个土坑中心 ![[公式]](https://www.zhihu.com/equation?tex=q_j) 都会有一个距离 ![[公式]](https://www.zhihu.com/equation?tex=d_%7Bij%7D) ，则构成了一个m*n的距离矩阵。

![[公式]](https://www.zhihu.com/equation?tex=D%3D%5Bd_%7Bij%7D%5D)

那么问题就是我们希望找到一个流（flow），当然也是个矩阵 ![[公式]](https://www.zhihu.com/equation?tex=f_%7Bij%7D) ，每一项 ![[公式]](https://www.zhihu.com/equation?tex=f_%7Bij%7D) 代表从 ![[公式]](https://www.zhihu.com/equation?tex=p_i) 到 ![[公式]](https://www.zhihu.com/equation?tex=q_j) 的流动数量，从而最小化整体的代价函数：

![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7BWORK%7D%28P%2CQ%2CF%29+%3D+%5Csum_%7Bi%3D1%7D%5Em%5Csum_%7Bj%3D1%7D%5End_%7Bij%7Df_%7Bij%7D)

问题描述清楚了：就是把P中的m个坑的土，用最小的代价搬到Q中的n个坑中， ![[公式]](https://www.zhihu.com/equation?tex=p_i) 到 ![[公式]](https://www.zhihu.com/equation?tex=q_j) 的两个坑的距离由 ![[公式]](https://www.zhihu.com/equation?tex=d_%7Bij%7D) 来表示。 ![[公式]](https://www.zhihu.com/equation?tex=f_%7Bij%7D) 是从 ![[公式]](https://www.zhihu.com/equation?tex=p_i) 搬到 ![[公式]](https://www.zhihu.com/equation?tex=q_j) 的土的量； ![[公式]](https://www.zhihu.com/equation?tex=d_%7Bij%7D) 是 ![[公式]](https://www.zhihu.com/equation?tex=p_i) 位置到 ![[公式]](https://www.zhihu.com/equation?tex=q_j) 位置的代价（距离）。要最小化WORK工作量。EMD是把这个工作量归一化以后的表达，即除以对 ![[公式]](https://www.zhihu.com/equation?tex=f_%7Bij%7D) 的求和。

![img](https://pic3.zhimg.com/80/v2-4d3234d9acddb97fb72c480a63f6360e_hd.jpg)

EMD公式：

![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7BEMD%7D%28P%2CQ%29+%3D+%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5EM%5Csum_%7Bj%3D1%7D%5ENd_%7Bij%7Df_%7Bij%7D%7D%7B%5Csum_%7Bi%3D1%7D%5EM%5Csum_%7Bj%3D1%7D%5ENf_%7Bij%7D%7D)

EMD 在深度学习中分类问题上的使用：

损失函数推导：

![img](https://pic4.zhimg.com/80/v2-f0c98a0a2010707879db1dbb30be214b_hd.jpg)

其中， ![[公式]](https://www.zhihu.com/equation?tex=CDF_p%28k%29+%3D+%5Csum_%7Bi%3D1%7D%5E%7Bk%7D%7Bp_%7Bsi%7D%7D) ，【也就是预测评分的概率的**累加值**，而不是独立的预测获得每一个评分的概率。以此代替**分布**。在 label 中，评分越高，累计概率越大。在 predictions 中，因为用了 softmax 确保每个独立概率都大于零 (且和为1)，因此 predictions 的累加概率也能随评分单调递增。】

